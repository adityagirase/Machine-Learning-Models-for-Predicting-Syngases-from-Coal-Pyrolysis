{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIRED LIBRARY INSTALLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from mpltern.ternary import TernaryAxes\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import shap\n",
    "import pickle\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'DataSets.xlsx'  # Update this with the correct path if needed\n",
    "data = pd.read_excel(file_path, sheet_name='DataSet')\n",
    "\n",
    "# Inspect data to ensure it’s loaded correctly\n",
    "print(data.head())\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set output directory\n",
    "output_dir = \"parameter_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize list to store statistics\n",
    "stats_summary = []\n",
    "\n",
    "# Filter numerical columns\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Compute statistics\n",
    "for column in numerical_columns:\n",
    "    col_data = data[column].dropna()\n",
    "    mean_val = col_data.mean()\n",
    "    median_val = col_data.median()\n",
    "    std_val = col_data.std()\n",
    "    q1 = np.percentile(col_data, 25)\n",
    "    q3 = np.percentile(col_data, 75)\n",
    "    iqr = q3 - q1\n",
    "    min_val = col_data.min()\n",
    "    max_val = col_data.max()\n",
    "\n",
    "    stats_summary.append({\n",
    "        'Column': column,\n",
    "        'Mean': mean_val,\n",
    "        'Median': median_val,\n",
    "        'Std Dev': std_val,\n",
    "        'Q1 (25%)': q1,\n",
    "        'Q3 (75%)': q3,\n",
    "        'IQR': iqr,\n",
    "        'Min': min_val,\n",
    "        'Max': max_val\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "stats_df = pd.DataFrame(stats_summary)\n",
    "stats_file_path = os.path.join(output_dir, \"summary_statistics_from_excel.csv\")\n",
    "stats_df.to_csv(stats_file_path, index=False)\n",
    "\n",
    "print(f\"\\nSummary statistics saved to {stats_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"parameter_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define column groups\n",
    "ultimate_columns = ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)']\n",
    "proximate_columns = ['Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)']\n",
    "process_columns = ['Sample (g)','Temperature (°C)', 'Heating rate (°Cmin-1)', 'Reaction time (min)']\n",
    "yield_columns = ['Syngas yield (%)', 'Char yield (%)', 'Pyrolysis oil yield (%)']\n",
    "gas_columns = ['CO2 (mol%)', 'CH4 (mol%)', 'CO (mol%)', 'H2 (mol%)']\n",
    "\n",
    "# Define colors for each group\n",
    "colors = {\n",
    "    'Ultimate': 'skyblue',\n",
    "    'Proximate': 'salmon',\n",
    "    'Process': 'lightgreen',\n",
    "    'Yield': 'orchid',\n",
    "    'Gas': 'gold'\n",
    "}\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    # Replace problematic characters with underscores\n",
    "    invalid_chars = ['<', '>', ':', '\"', '/', '\\\\', '|', '?', '*', '°', '%', '(', ')', ' ']\n",
    "    filename = name\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    # Remove any duplicate underscores\n",
    "    while '__' in filename:\n",
    "        filename = filename.replace('__', '_')\n",
    "    return filename\n",
    "\n",
    "# Function to create half violin plot with box plot\n",
    "def half_violin_box_plot(data, column, color, category):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Calculate statistics for consistent y positioning\n",
    "    min_val = data[column].min() \n",
    "    max_val = data[column].max()\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    # Add padding to y-axis\n",
    "    y_min = min_val - range_val * 0.1\n",
    "    y_max = max_val + range_val * 0.1\n",
    "    \n",
    "    # Define positions for the violin and box plots\n",
    "    violin_pos = 0.3\n",
    "    box_pos = 0.7\n",
    "    \n",
    "    # Create violin plot\n",
    "    violin_parts = ax.violinplot(data[column], positions=[violin_pos], \n",
    "                           vert=True, widths=0.3, showmeans=False, \n",
    "                           showextrema=False, showmedians=False)\n",
    "    \n",
    "    # Customize violin plot - keep only the left half\n",
    "    for pc in violin_parts['bodies']:\n",
    "        verts = pc.get_paths()[0].vertices\n",
    "        # Only keep the left half of the violin\n",
    "        verts[:, 0] = np.clip(verts[:, 0], violin_pos - 0.15, violin_pos)\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.8)\n",
    "    \n",
    "    # Add box plot on the right\n",
    "    box_parts = ax.boxplot(data[column], positions=[box_pos], \n",
    "                     vert=True, widths=0.3, patch_artist=True, \n",
    "                     showcaps=True, showfliers=True)\n",
    "    \n",
    "    # Customize box plot\n",
    "    for component in ['boxes', 'whiskers', 'caps', 'medians']:\n",
    "        for patch in box_parts[component]:\n",
    "            patch.set_color('black')\n",
    "    \n",
    "    for patch in box_parts['boxes']:\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    # Add reference lines for mean and median\n",
    "    mean_val = data[column].mean()\n",
    "    median_val = data[column].median()\n",
    "    \n",
    "    ax.axhline(mean_val, color='red', linestyle='-', linewidth=1.5, alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axhline(median_val, color='blue', linestyle='--', linewidth=1.5, alpha=0.7, label=f'Median: {median_val:.2f}')\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    std_val = data[column].std()\n",
    "    q1 = np.percentile(data[column], 25)\n",
    "    q3 = np.percentile(data[column], 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Add statistics as text\n",
    "    stats_text = (\n",
    "        f\"Mean: {mean_val:.2f}\\n\"\n",
    "        f\"Median: {median_val:.2f}\\n\"\n",
    "        f\"Std Dev: {std_val:.2f}\\n\"\n",
    "        f\"IQR: {iqr:.2f}\\n\"\n",
    "        f\"Range: {min_val:.2f} - {max_val:.2f}\"\n",
    "    )\n",
    "    \n",
    "    # Position text in the upper right\n",
    "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_title(f\"{category} - {column}\", fontsize=14)\n",
    "    ax.set_ylabel(column, fontsize=12)\n",
    "    \n",
    "    # Remove x-ticks since they're not meaningful\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "    # Set y-axis limits with padding\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add grid for readability\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    # Create safe filename\n",
    "    safe_category = sanitize_filename(category)\n",
    "    safe_column = sanitize_filename(column)\n",
    "    filename = f\"half_violin_box_{safe_category}_{safe_column}.png\"\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Process all columns\n",
    "column_groups = {\n",
    "    'Ultimate': ultimate_columns,\n",
    "    'Proximate': proximate_columns,\n",
    "    'Process': process_columns,\n",
    "    'Yield': yield_columns,\n",
    "    'Gas': gas_columns\n",
    "}\n",
    "\n",
    "# List to store created filenames\n",
    "created_files = []\n",
    "\n",
    "# Create individual plots for each column\n",
    "for category, columns in column_groups.items():\n",
    "    for column in columns:\n",
    "        if column in data.columns:  # Assumes 'data' dataframe is defined elsewhere\n",
    "            try:\n",
    "                filename = half_violin_box_plot(data, column, colors[category], category)\n",
    "                created_files.append(filename)\n",
    "                print(f\"Created half violin plot with box plot for {column}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating plot for {column}: {e}\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nAnalysis complete. Created {len(created_files)} half violin plots with box plots.\")\n",
    "print(f\"All plots saved to {output_dir} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEARSON PLOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the features - properly formatted as a list of strings\n",
    "features = [\n",
    "    'C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)', \n",
    "    'Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)',\n",
    "    'Sample (g)', 'Temperature (°C)', 'Heating rate (°Cmin-1)',\n",
    "    'Reaction time (min)', 'Syngas yield (%)',\n",
    "    'CH4 (mol%)', 'H2 (mol%)', 'Char yield (%)',\n",
    "    'Pyrolysis oil yield (%)'\n",
    "]\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"correlation_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Filter dataset to include only the selected features\n",
    "# Remove the 'Sample' column if it's non-numeric to avoid correlation errors\n",
    "numeric_features = [f for f in features if f != 'Sample']\n",
    "data_filtered = data[numeric_features]\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = data_filtered.corr(method='pearson')\n",
    "\n",
    "# Create a mask for the upper triangle to avoid redundancy\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Plot heatmap with improved styling\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.set(font_scale=1.2)  # Increase font size for better readability\n",
    "\n",
    "# Create the heatmap with better styling\n",
    "heatmap = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True,               # Show correlation values\n",
    "    cmap=\"coolwarm\",          # Use a diverging colormap\n",
    "    fmt=\".2f\",                # Format numbers to 2 decimal places\n",
    "    linewidths=0.5,           # Add grid lines                # Apply mask to show only lower triangle\n",
    "    cbar_kws={\"shrink\": 0.8}, # Adjust colorbar size\n",
    "    annot_kws={\"size\": 10}    # Adjust annotation text size\n",
    ")\n",
    "\n",
    "# Improve the visualization\n",
    "plt.title(\"Pearson Correlation Heatmap of Pyrolysis Parameters\", fontsize=18, pad=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x labels for better visibility\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y labels horizontal\n",
    "plt.tight_layout()                                # Adjust layout to fit everything\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(output_dir, \"pearson_correlation_heatmap.png\"), dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation heatmap saved to {os.path.join(output_dir, 'pearson_correlation_heatmap.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input variables and output\n",
    "syngas_yield = data['Syngas yield (%)']\n",
    "char_yield = data['Char yield (%)']\n",
    "pyrolysis_oil_yield = data['Pyrolysis oil yield (%)']\n",
    "temperature = data['Temperature (°C)']  # Output variable (this will be the color)\n",
    "\n",
    "# Normalize the ternary inputs to sum to 1\n",
    "total = syngas_yield + char_yield + pyrolysis_oil_yield\n",
    "syngas_norm = syngas_yield / total\n",
    "char_norm = char_yield / total\n",
    "oil_norm = pyrolysis_oil_yield / total\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(10.8, 4.8))\n",
    "fig.subplots_adjust(left=0.075, right=0.85, wspace=0.3)\n",
    "\n",
    "# Set color scale range\n",
    "vmin = temperature.min()\n",
    "vmax = temperature.max()\n",
    "levels = np.linspace(vmin, vmax, 7)\n",
    "\n",
    "# First subplot with flat shading\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='ternary')\n",
    "cs1 = ax1.tripcolor(syngas_norm, char_norm, oil_norm, temperature, \n",
    "                   shading='flat', vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax1.set_tlabel('Syngas Yield')\n",
    "ax1.set_llabel('Char Yield')\n",
    "ax1.set_rlabel('Pyrolysis Oil Yield')\n",
    "\n",
    "# Add colorbar to first subplot\n",
    "cax1 = ax1.inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax1.transAxes)\n",
    "colorbar1 = fig.colorbar(cs1, cax=cax1)\n",
    "colorbar1.set_label('Temperature (°C)', rotation=270, va='baseline')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(output_dir, \"Syngas Yield.png\"), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation heatmap saved to {os.path.join(output_dir, 'Syngas Yield.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SYNGAS + PYROLYSIS OIL + CHAR (UA+OC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=2000,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.012,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.13,\n",
    "        reg_alpha=0.65,\n",
    "        reg_lambda=0.85,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['Syngas yield (%)', 'Char yield (%)','Pyrolysis oil yield (%)']\n",
    "feature_sets = {\n",
    "    \"Ultimate Analysis + Operating Conditions\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)', 'Sample (g)', 'Temperature (°C)', 'Reaction time (min)', 'Heating rate (°Cmin-1)'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X, y = data[feature_cols], data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    with open(f\"{name.replace(' ', '_').lower()}_xgboost_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"shap_plots\"):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    \n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract the individual estimator for this target (for multi-output models)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_plot_path = os.path.join(output_dir, f\"{model_name}_{target}_shap_dot.png\")\n",
    "        plt.savefig(dot_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 2: SHAP Summary Bar Plot\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_plot_path = os.path.join(output_dir, f\"{model_name}_{target}_shap_bar.png\")\n",
    "        plt.savefig(bar_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Saved plots to:\\n  - {dot_plot_path}\\n  - {bar_plot_path}\")\n",
    "\n",
    "X = data[feature_sets[\"Ultimate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Ultimate Analysis + Operating Conditions\"], X, targets, model_name=\"Ultimate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SYNGAS + PYROLYSIS OIL + CHAR (PA+OC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=900,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.12,\n",
    "        subsample=0.3,\n",
    "        colsample_bytree=0.285,\n",
    "        reg_alpha=0.7,\n",
    "        reg_lambda=0.7,\n",
    "        random_state=29\n",
    "    )\n",
    "\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['Syngas yield (%)', 'Char yield (%)','Pyrolysis oil yield (%)']\n",
    "feature_sets = {\n",
    "    \n",
    "    \"Proximate Analysis + Operating Conditions\": ['Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)', 'Sample (g)', 'Temperature (°C)', 'Reaction time (min)', 'Heating rate (°Cmin-1)'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X, y = data[feature_cols], data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    with open(f\"{name.replace(' ', '_').lower()}_xgboost_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"shap_plots\"):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    \n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract the individual estimator for this target (for multi-output models)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_plot_path = os.path.join(output_dir, f\"{model_name}_{target}_shap_dot.png\")\n",
    "        plt.savefig(dot_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 2: SHAP Summary Bar Plot\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_plot_path = os.path.join(output_dir, f\"{model_name}_{target}_shap_bar.png\")\n",
    "        plt.savefig(bar_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Saved plots to:\\n  - {dot_plot_path}\\n  - {bar_plot_path}\")\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Proximate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Proximate Analysis + Operating Conditions\"], X, targets, model_name=\"Proximate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SYNGAS + PYROLYSIS OIL + CHAR (UA+PA+OC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=900,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.02,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['Syngas yield (%)', 'Char yield (%)','Pyrolysis oil yield (%)']\n",
    "feature_sets = {\n",
    "    \"Ultimate + Proximate Analysis + Operating Conditions\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)', 'Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)', 'Sample (g)', 'Temperature (°C)', 'Reaction time (min)', 'Heating rate (°Cmin-1)']\n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X, y = data[feature_cols], data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # Save model to .pkl\n",
    "    with open(f\"{name.replace(' ', '_').lower()}_xgboost_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"shap_plots\"):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    \n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract the individual estimator for this target (for multi-output models)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_plot_path = os.path.join(output_dir, f\"{model_name}_{target}_shap_dot.png\")\n",
    "        plt.savefig(dot_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 2: SHAP Summary Bar Plot\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_plot_path = os.path.join(output_dir, f\"{model_name}_{target}_shap_bar.png\")\n",
    "        plt.savefig(bar_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Saved plots to:\\n  - {dot_plot_path}\\n  - {bar_plot_path}\")\n",
    "\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Ultimate + Proximate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Ultimate + Proximate Analysis + Operating Conditions\"], X, targets, model_name=\"Ultimate + Proximate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_and_evaluate(X, y, model_type=\"XGBoost\", model_name=\"Model\", n_neighbors=5):\n",
    "    # Split the dataset into training and testing sets (80-20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=27)\n",
    "\n",
    "    # Initialize the model based on the model_type parameter\n",
    "    if model_type == \"DecisionTree\":\n",
    "        regressor = DecisionTreeRegressor(random_state=42)\n",
    "    elif model_type == \"AdaBoost\":\n",
    "        regressor = AdaBoostRegressor(estimator=DecisionTreeRegressor(random_state=42), random_state=42)\n",
    "    elif model_type == \"XGBoost\":\n",
    "        regressor = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "    elif model_type == \"GradientBoosting\":\n",
    "        regressor = GradientBoostingRegressor(random_state=42)\n",
    "    elif model_type == \"KNN\":\n",
    "        regressor = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    elif model_type == \"RandomForest\":\n",
    "        regressor = RandomForestRegressor(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    # Train the model\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the training and test sets\n",
    "    y_train_pred = regressor.predict(X_train)\n",
    "    y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"\\n{model_name} ({model_type}):\")\n",
    "    # print(f\"Training Set Mean Squared Error: {train_mse}\")\n",
    "    # print(f\"Training Set Mean Absolute Error: {train_mae}\")\n",
    "    print(f\"Training Set R² Score: {train_r2}\")\n",
    "    # print(f\"Test Set Mean Squared Error: {test_mse}\")\n",
    "    # print(f\"Test Set Mean Absolute Error: {test_mae}\")\n",
    "    print(f\"Test Set R² Score: {test_r2}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define target variables\n",
    "targets = {\n",
    "    'CH4 (mol%)': 'Y_CH'\n",
    "}\n",
    "features = {\n",
    "    \"Ultimate Analysis\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)'],\n",
    "    \"Proximate Analysis\": ['Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)'],\n",
    "    \"Ultimate + Proximate Analysis + Operating Conditions\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)', 'Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)']\n",
    "}\n",
    "\n",
    "# List of model types\n",
    "model_types = [\"DecisionTree\", \"AdaBoost\", \"XGBoost\", \"GradientBoosting\", \"KNN\", \"RandomForest\"]\n",
    "\n",
    "# Iterate over targets, features, and model types\n",
    "for target, target_name in targets.items():\n",
    "    y = data[target]\n",
    "    for feature_set_name, features_list in features.items():\n",
    "        print(features_list)\n",
    "        X = data[features_list]\n",
    "        train_and_evaluate(X, y, model_type=\"XGBoost\", model_name=feature_set_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH4 (UA+PA+OC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "# -------------------- Model Config --------------------\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=400,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.13,\n",
    "        subsample=0.5,\n",
    "        colsample_bytree=0.19,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.4,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "# -------------------- Training and Combined Parity Plot --------------------\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # ----- Combined Parity Plot -----\n",
    "    target = y.columns[0]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_train_pred, label=f\"Train (R² = {train_r2:.3f})\", color=\"Red\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.scatter(y_test, y_test_pred, label=f\"Test (R² = {test_r2:.3f})\", color=\"Blue\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y.min().values[0], y.max().values[0]], [y.min().values[0], y.max().values[0]], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'{model_name} - Parity Plot for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "# Make sure your DataFrame `data` is defined\n",
    "data = preprocess_data(data)\n",
    "\n",
    "targets = ['CH4 (mol%)']\n",
    "feature_sets = {\n",
    "    \"Ultimate + Proximate Analysis + Operating Conditions\": [\n",
    "        'C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)',\n",
    "        'Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)', 'Sample (g)',\n",
    "        'Temperature (°C)', 'Reaction time (min)', 'Heating rate (°Cmin-1)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X = data[feature_cols]\n",
    "    y = data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # --- Save model to .pkl file ---\n",
    "    file_name = f\"{name.replace(' ', '_').lower()}ch4_xgboost_model.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"SHAP_plots_CH4\", save_bar_plot=False):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract individual estimator (for multi-output model)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # SHAP Explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_path = os.path.join(output_dir, f\"{model_name}_{target}_dot.png\")\n",
    "        plt.savefig(dot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Dot plot saved to: {dot_path}\")\n",
    "\n",
    "        \n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_path = os.path.join(output_dir, f\"{model_name}_{target}_bar.png\")\n",
    "        plt.savefig(bar_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Bar plot saved to: {bar_path}\")\n",
    "\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Ultimate + Proximate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Ultimate + Proximate Analysis + Operating Conditions\"], X, targets, model_name=\"Ultimate + Proximate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH4 (PA+OC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=700,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.3,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.4,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "# -------------------- Training and Combined Parity Plot --------------------\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # ----- Combined Parity Plot -----\n",
    "    target = y.columns[0]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_train_pred, label=f\"Train (R² = {train_r2:.3f})\", color=\"Red\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.scatter(y_test, y_test_pred, label=f\"Test (R² = {test_r2:.3f})\", color=\"Blue\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y.min().values[0], y.max().values[0]], [y.min().values[0], y.max().values[0]], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'{model_name} - Parity Plot for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['CH4 (mol%)']\n",
    "feature_sets = {\n",
    "    \"Proximate Analysis + Operating Conditions\": ['Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)'],\n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X = data[feature_cols]\n",
    "    y = data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # --- Save model to .pkl file ---\n",
    "    file_name = f\"{name.replace(' ', '_').lower()}_ch4_xgboost_model.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"SHAP_plots_CH4\", save_bar_plot=False):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract individual estimator (for multi-output model)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # SHAP Explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_path = os.path.join(output_dir, f\"{model_name}_{target}_dot.png\")\n",
    "        plt.savefig(dot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Dot plot saved to: {dot_path}\")\n",
    "\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_path = os.path.join(output_dir, f\"{model_name}_{target}_bar.png\")\n",
    "        plt.savefig(bar_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Bar plot saved to: {bar_path}\")\n",
    "\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Proximate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Proximate Analysis + Operating Conditions\"], X, targets, model_name=\"Proximate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH4 (UA+OC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=900,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.5,\n",
    "        colsample_bytree=0.3,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.4,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "# -------------------- Training and Combined Parity Plot --------------------\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # ----- Combined Parity Plot -----\n",
    "    target = y.columns[0]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_train_pred, label=f\"Train (R² = {train_r2:.3f})\", color=\"Red\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.scatter(y_test, y_test_pred, label=f\"Test (R² = {test_r2:.3f})\", color=\"Blue\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y.min().values[0], y.max().values[0]], [y.min().values[0], y.max().values[0]], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'{model_name} - Parity Plot for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['CH4 (mol%)']\n",
    "feature_sets = {\n",
    "    \"Ultimate Analysis + Operating Conditions\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)'],\n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X = data[feature_cols]\n",
    "    y = data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # --- Save model to .pkl file ---\n",
    "    file_name = f\"{name.replace(' ', '_').lower()}_ch4_xgboost_model.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"SHAP_plots_CH4\", save_bar_plot=False):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract individual estimator (for multi-output model)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # SHAP Explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_path = os.path.join(output_dir, f\"{model_name}_{target}_dot.png\")\n",
    "        plt.savefig(dot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Dot plot saved to: {dot_path}\")\n",
    "\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_path = os.path.join(output_dir, f\"{model_name}_{target}_bar.png\")\n",
    "        plt.savefig(bar_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Bar plot saved to: {bar_path}\")\n",
    "\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Ultimate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Ultimate Analysis + Operating Conditions\"], X, targets, model_name=\"Ultimate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H2 (UA+OC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=1100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.0199,\n",
    "        subsample=0.1,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.05,\n",
    "        reg_lambda=0.8,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "# -------------------- Training and Combined Parity Plot --------------------\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # ----- Combined Parity Plot -----\n",
    "    target = y.columns[0]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_train_pred, label=f\"Train (R² = {train_r2:.3f})\", color=\"Red\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.scatter(y_test, y_test_pred, label=f\"Test (R² = {test_r2:.3f})\", color=\"Blue\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y.min().values[0], y.max().values[0]], [y.min().values[0], y.max().values[0]], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'{model_name} - Parity Plot for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['H2 (mol%)']\n",
    "feature_sets = {\n",
    "    \"Ultimate Analysis + Operating Conditions\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)'],\n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X = data[feature_cols]\n",
    "    y = data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # --- Save model to .pkl file ---\n",
    "    file_name = f\"{name.replace(' ', '_').lower()}_h2_xgboost_model.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"SHAP_plots_H2\", save_bar_plot=False):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract individual estimator (for multi-output model)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # SHAP Explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_path = os.path.join(output_dir, f\"{model_name}_{target}_dot.png\")\n",
    "        plt.savefig(dot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Dot plot saved to: {dot_path}\")\n",
    "\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_path = os.path.join(output_dir, f\"{model_name}_{target}_bar.png\")\n",
    "        plt.savefig(bar_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Bar plot saved to: {bar_path}\")\n",
    "\n",
    "            \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Ultimate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Ultimate Analysis + Operating Conditions\"], X, targets, model_name=\"Ultimate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H2 (PA+OC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=1100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.1,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.05,\n",
    "        reg_lambda=0.8,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "# -------------------- Training and Combined Parity Plot --------------------\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # ----- Combined Parity Plot -----\n",
    "    target = y.columns[0]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_train_pred, label=f\"Train (R² = {train_r2:.3f})\", color=\"Red\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.scatter(y_test, y_test_pred, label=f\"Test (R² = {test_r2:.3f})\", color=\"Blue\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y.min().values[0], y.max().values[0]], [y.min().values[0], y.max().values[0]], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'{model_name} - Parity Plot for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['H2 (mol%)']\n",
    "feature_sets = {\n",
    "    \"Proximate Analysis + Operating Conditions\": ['Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°Cmin-1)'],\n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X = data[feature_cols]\n",
    "    y = data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # --- Save model to .pkl file ---\n",
    "    file_name = f\"{name.replace(' ', '_').lower()}_h2_xgboost_model.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"SHAP_plots_H2\", save_bar_plot=False):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract individual estimator (for multi-output model)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # SHAP Explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_path = os.path.join(output_dir, f\"{model_name}_{target}_dot.png\")\n",
    "        plt.savefig(dot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Dot plot saved to: {dot_path}\")\n",
    "\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_path = os.path.join(output_dir, f\"{model_name}_{target}_bar.png\")\n",
    "        plt.savefig(bar_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Bar plot saved to: {bar_path}\")\n",
    "\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[\"Proximate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[\"Proximate Analysis + Operating Conditions\"], X, targets, model_name=\"Proximate Analysis + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H2 (UA+PA+OC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Encode 'Sample' column if present\n",
    "    if 'Sample' in data.columns:\n",
    "        data['Sample'] = data['Sample'].astype('category').cat.codes\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=800,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.2,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.4,\n",
    "        reg_lambda=0.9,\n",
    "        random_state=27\n",
    "    )\n",
    "\n",
    "# -------------------- Training and Combined Parity Plot --------------------\n",
    "def train_xgboost_multi(X, y, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=27)\n",
    "\n",
    "    model = MultiOutputRegressor(get_model())\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')\n",
    "    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')\n",
    "\n",
    "    print(f\"\\n{model_name} (XGBoost)\")\n",
    "    print(f\"Training Set R² Score: {train_r2:.4f}\")\n",
    "    print(f\"Test Set R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # ----- Combined Parity Plot -----\n",
    "    target = y.columns[0]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_train, y_train_pred, label=f\"Train (R² = {train_r2:.3f})\", color=\"Red\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.scatter(y_test, y_test_pred, label=f\"Test (R² = {test_r2:.3f})\", color=\"Blue\", alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y.min().values[0], y.max().values[0]], [y.min().values[0], y.max().values[0]], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'{model_name} - Parity Plot for {target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Ensure your dataframe `data` is defined before this\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define targets and feature sets\n",
    "targets = ['H2 (mol%)']\n",
    "feature_sets = {\n",
    "    \"Ultimate + Proximate Analysis + Operating Conditions\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)', 'Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)', 'Sample (g)', 'Temperature (°C)', 'Reaction time (min)', 'Heating rate (°Cmin-1)'],\n",
    "}\n",
    "\n",
    "# Train and store models\n",
    "trained_models = {}\n",
    "for name, feature_cols in feature_sets.items():\n",
    "    X = data[feature_cols]\n",
    "    y = data[targets]\n",
    "    model = train_xgboost_multi(X, y, model_name=name)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # --- Save model to .pkl file ---\n",
    "    file_name = f\"{name.replace(' ', '_').lower()}_h2_xgboost_model.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_analysis(model, X, target_names, model_name=\"Model\", output_dir=\"SHAP_plots_H2\", save_bar_plot=False):\n",
    "    print(f\"\\n🔍 SHAP Analysis for {model_name}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    for i, target in enumerate(target_names):\n",
    "        print(f\"\\n📊 SHAP Feature Importance for Target: {target}\")\n",
    "\n",
    "        # Extract individual estimator (for multi-output model)\n",
    "        xgb_model = model.estimators_[i]\n",
    "\n",
    "        # SHAP Explainer\n",
    "        explainer = shap.Explainer(xgb_model, X)\n",
    "        shap_values = explainer(X)\n",
    "\n",
    "        # Plot 1: SHAP Summary Dot Plot\n",
    "        print(\"🔹 SHAP Summary Dot Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"dot\", show=False)\n",
    "        plt.title(f\"SHAP Dot Plot - {model_name} - {target}\")\n",
    "        dot_path = os.path.join(output_dir, f\"{model_name}_{target}_dot.png\")\n",
    "        plt.savefig(dot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Dot plot saved to: {dot_path}\")\n",
    "\n",
    "        print(\"🔹 SHAP Summary Bar Plot:\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "        plt.title(f\"SHAP Bar Plot - {model_name} - {target}\")\n",
    "        bar_path = os.path.join(output_dir, f\"{model_name}_{target}_bar.png\")\n",
    "        plt.savefig(bar_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Bar plot saved to: {bar_path}\")\n",
    "\n",
    "        \n",
    "# Run SHAP for \"Ultimate Analysis\"\n",
    "X = data[feature_sets[ \"Ultimate + Proximate Analysis + Operating Conditions\"]]\n",
    "shap_analysis(trained_models[ \"Ultimate + Proximate Analysis + Operating Conditions\"], X, targets, model_name= \"Ultimate + Proximate Analysis + + Operating Conditions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRONT END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import joblib\n",
    "\n",
    "# Paths to models\n",
    "model_paths = {\n",
    "    \"All Parameters\": \"ultimate_+_proximate_analysis_+_operating_conditions_xgboost_model.pkl\",\n",
    "    \"Proximate & Operating\": \"proximate_analysis_+_operating_conditions_xgboost_model.pkl\",\n",
    "    \"UA & Operating\": \"ultimate_analysis_+_operating_conditions_xgboost_model.pkl\",\n",
    "    \"All Parameters CH4\": \"ultimate_+_proximate_analysis_+_operating_conditionsch4_xgboost_model.pkl\",\n",
    "    \"Proximate & Operating CH4\": \"proximate_analysis_+_operating_conditions_ch4_xgboost_model.pkl\",\n",
    "    \"UA & Operating CH4\": \"ultimate_analysis_+_operating_conditions_ch4_xgboost_model.pkl\",\n",
    "    \"All Parameters H2\": \"ultimate_+_proximate_analysis_+_operating_conditions_h2_xgboost_model.pkl\",\n",
    "    \"Proximate & Operating H2\": \"proximate_analysis_+_operating_conditions_h2_xgboost_model.pkl\",\n",
    "    \"UA & Operating H2\": \"ultimate_analysis_+_operating_conditions_h2_xgboost_model.pkl\",\n",
    "}\n",
    "\n",
    "def load_model(path_key):\n",
    "    try:\n",
    "        return joblib.load(model_paths[path_key])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model '{path_key}': {e}\")\n",
    "        return None\n",
    "\n",
    "input_features = {\n",
    "    \"All Parameters\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)', 'Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°C/min)'],\n",
    "    \"PA & Operating\": ['Volatile (wt%)', 'Fixed carbon (wt%)', 'Ash (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°C/min)'],\n",
    "    \"UA & Operating\": ['C (wt%)', 'H (wt%)', 'N (wt%)', 'O (wt%)', 'S (wt%)','Sample (g)', 'Temperature (°C)', 'Reaction time (min)','Heating rate (°C/min)']\n",
    "}\n",
    "\n",
    "output_features = ['Syngas yield (%)',  'Char yield (%)', 'Pyrolysis oil yield (%)', 'CH4 (mol%)','H2 (mol%)']\n",
    "\n",
    "# Custom prediction function to combine models\n",
    "def biomass_prediction(input_category, input_values):\n",
    "    input_data = np.array(input_values).reshape(1, -1)\n",
    "    \n",
    "    if np.any(input_data < 0):\n",
    "        return [\"Invalid input\" for _ in output_features]\n",
    "    if np.all(input_data == 0):\n",
    "        return [\"0 %\" for _ in output_features]\n",
    "\n",
    "    result = [\"\"] * len(output_features)\n",
    "\n",
    "    # Predict main outputs\n",
    "    if input_category == \"PA & Operating\":\n",
    "        base_model = load_model(\"PA & Operating\")\n",
    "        ch4_model = load_model(\"PA & Operating CH4\")\n",
    "        h2_model = load_model(\"PA & Operating H2\")\n",
    "    elif input_category == \"UA & Operating\":\n",
    "        base_model = load_model(\"UA & Operating\")\n",
    "        ch4_model = load_model(\"UA & Operating CH4\")\n",
    "        h2_model = load_model(\"UA & Operating H2\")\n",
    "    elif input_category == \"All Parameters\":\n",
    "        base_model = load_model(\"All Parameters\")\n",
    "        ch4_model = load_model(\"All Parameters CH4\")\n",
    "        h2_model = load_model(\"All Parameters H2\")\n",
    "    else:\n",
    "        return [\"Invalid category\"] * len(output_features)\n",
    "\n",
    "    try:\n",
    "        base_pred = base_model.predict(input_data)[0]\n",
    "        h2_pred = h2_model.predict(input_data)[0]\n",
    "        ch4_pred = ch4_model.predict(input_data)[0]\n",
    "        \n",
    "        # Assign predictions\n",
    "        result[0] = f\"{round(base_pred[0], 2)} %\"\n",
    "        result[1] = f\"{round(base_pred[1], 2)} %\"\n",
    "        result[2] = f\"{round(base_pred[2], 2)} %\"\n",
    "        result[3] = f\"{round(ch4_pred[0], 2)} %\"\n",
    "        result[4] = f\"{round(h2_pred[0], 2)} %\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Prediction error:\", e)\n",
    "        result = [\"Error\"] * len(output_features)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Build GUI\n",
    "def create_gui():\n",
    "    with gr.Blocks(title=\"Coal Pyrolysis Prediction\", theme=gr.themes.Monochrome()) as interface:\n",
    "        gr.Markdown(\"# Coal Pyrolysis Prediction Tool\")\n",
    "        gr.Markdown(\n",
    "            \"This tool predicts pyrolysis product yields from coal. Choose the input type and provide values.\"\n",
    "        )\n",
    "\n",
    "        with gr.Tabs() as tabs:\n",
    "            for tab_name in [\"All Parameters\", \"PA & Operating\", \"UA & Operating\"]:\n",
    "                with gr.TabItem(tab_name):\n",
    "                    input_boxes = []\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            for i, feat in enumerate(input_features[tab_name]):\n",
    "                                if i < len(input_features[tab_name]) // 2 + 1:\n",
    "                                    input_boxes.append(gr.Number(label=feat, value=0))\n",
    "                        with gr.Column():\n",
    "                            for i, feat in enumerate(input_features[tab_name]):\n",
    "                                if i >= len(input_features[tab_name]) // 2 + 1:\n",
    "                                    input_boxes.append(gr.Number(label=feat, value=0))\n",
    "\n",
    "                    btn = gr.Button(f\"Predict\", variant=\"primary\")\n",
    "                    outputs = [gr.Textbox(label=feat) for feat in output_features]\n",
    "\n",
    "                    btn.click(\n",
    "                        fn=lambda *inputs, cat=tab_name: biomass_prediction(cat, inputs),\n",
    "                        inputs=input_boxes,\n",
    "                        outputs=outputs\n",
    "                    )\n",
    "    \n",
    "    interface.launch(inline=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_gui()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
